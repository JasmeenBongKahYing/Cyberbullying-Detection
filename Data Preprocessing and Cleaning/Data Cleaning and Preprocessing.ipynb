{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a1ce83",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494aa141",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d81dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Begin Python Imports\n",
    "import datetime, warnings, scipy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Text Cleaning & Normalization\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "print(spacy.__version__)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d2e0b",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ddde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "bully_data = pd.read_csv('bully_data_toclean_version.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec33154",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670fd28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123548 rows and 8 columns from the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Check dimension of dataset\n",
    "bully_data.shape\n",
    "print(\"There are \"+ str(bully_data.shape[0]) +\" rows and \"+ str(bully_data.shape[1]) +\" columns from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd79acc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123548 entries, 0 to 123547\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Unnamed: 0         123548 non-null  int64  \n",
      " 1   tag                123548 non-null  object \n",
      " 2   text               123548 non-null  object \n",
      " 3   label              5376 non-null    object \n",
      " 4   role               5376 non-null    object \n",
      " 5   harmfulness_score  5376 non-null    float64\n",
      " 6   oth_language       8754 non-null    float64\n",
      " 7   file_index         123548 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check column type\n",
    "bully_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4b9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unwanted column\n",
    "bully_data.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9929be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unwanted column\n",
    "bully_data=bully_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14006f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123548 entries, 0 to 123547\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   tag                123548 non-null  object \n",
      " 1   text               123548 non-null  object \n",
      " 2   label              5376 non-null    object \n",
      " 3   role               5376 non-null    object \n",
      " 4   harmfulness_score  5376 non-null    float64\n",
      " 5   oth_language       8754 non-null    float64\n",
      " 6   file_index         123548 non-null  object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Last check column type\n",
    "bully_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c2c4c",
   "metadata": {},
   "source": [
    "## Check if got missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170c35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing data in columns\n",
      "         column_name  percentage\n",
      "0              label   95.648655\n",
      "1               role   95.648655\n",
      "2  harmfulness_score   95.648655\n",
      "3       oth_language   92.914495\n"
     ]
    }
   ],
   "source": [
    "# Calculate the proportion of missing data\n",
    "\n",
    "def checkMissing(data,perc=0):\n",
    "    \"\"\" \n",
    "    Function that takes in a dataframe and returns\n",
    "    the percentage of missing value.\n",
    "    \"\"\"\n",
    "    missing = [(i, data[i].isna().mean()*100) for i in data]\n",
    "    missing = pd.DataFrame(missing, columns=[\"column_name\", \"percentage\"])\n",
    "    missing = missing[missing.percentage > perc]\n",
    "    print(missing.sort_values(\"percentage\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "print(\"Proportion of missing data in columns\")\n",
    "checkMissing(bully_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b61d3a",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70dda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_text as pt\n",
    "import language_tool_python\n",
    "import gensim.downloader as api\n",
    "#from pycontractions.contractions import Contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "glove_model = api.load(\"glove-twitter-100\")\n",
    "\n",
    "# Functions\n",
    "def expand_contractions_with_embeddings(text, embeddings_model):\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if \"'\" in word and word.lower() != \"n't\":\n",
    "            contraction = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "            \n",
    "            # Check if the contraction is present in the vocabulary\n",
    "            if contraction in embeddings_model:\n",
    "                expansion_candidates = embeddings_model.most_similar(positive=[contraction], topn=5)\n",
    "                best_expansion = expansion_candidates[0][0]\n",
    "                expanded_words.append(best_expansion)\n",
    "            else:\n",
    "                # Handle common variations of contractions\n",
    "                if contraction == \"i\":\n",
    "                    expanded_words.append(\"I\")\n",
    "                elif contraction == \"youre\":\n",
    "                    expanded_words.append(\"you're\")\n",
    "                else:\n",
    "                    expanded_words.append(word)\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def get_term_list(path):\n",
    "    '''\n",
    "    Function to import term list file\n",
    "    '''\n",
    "    word_list = []\n",
    "    with open(path,\"r\") as f:\n",
    "        for line in f:\n",
    "            word = line.replace(\"\\n\",\"\").strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    '''\n",
    "    Function returns unique words in document corpus\n",
    "    '''\n",
    "    # vocab set\n",
    "    unique_words = set()\n",
    "    \n",
    "    # looping through each document in corpus\n",
    "    for document in tqdm(corpus):\n",
    "        for word in document.split(\" \"):\n",
    "            if len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "def create_profane_mapping(profane_words,vocabulary):\n",
    "    '''\n",
    "    Function creates a mapping between commonly found profane words and words in \n",
    "    document corpus \n",
    "    '''\n",
    "    \n",
    "    # mapping dictionary\n",
    "    mapping_dict = dict()\n",
    "    \n",
    "    # looping through each profane word\n",
    "    for profane in tqdm(profane_words):\n",
    "        mapped_words = set()\n",
    "        \n",
    "        # looping through each word in vocab\n",
    "        for word in vocabulary:\n",
    "            # mapping only if ratio > 80\n",
    "            try:\n",
    "                if fuzz.ratio(profane,word) > 90:\n",
    "                    mapped_words.add(word)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # list of all vocab words for given profane word\n",
    "        mapping_dict[profane] = mapped_words\n",
    "    \n",
    "    return mapping_dict\n",
    "\n",
    "def replace_words(corpus,mapping_dict):\n",
    "    '''\n",
    "    Function replaces obfuscated profane words using a mapping dictionary\n",
    "    '''\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # iterating over each document in the corpus\n",
    "    for document in tqdm(corpus):\n",
    "        \n",
    "        # splitting sentence to word\n",
    "        comment = document.split()\n",
    "        \n",
    "        # iterating over mapping_dict\n",
    "        for mapped_word,v in mapping_dict.items():\n",
    "            \n",
    "            # comparing target word to each comment word \n",
    "            for target_word in v:\n",
    "                \n",
    "                # each word in comment\n",
    "                for i,word in enumerate(comment):\n",
    "                    if word == target_word:\n",
    "                        comment[i] = mapped_word\n",
    "        \n",
    "        # joining comment words\n",
    "        document = \" \".join(comment)\n",
    "        document = document.strip()\n",
    "                    \n",
    "        processed_corpus.append(document)\n",
    "        \n",
    "    return processed_corpus\n",
    "\n",
    "# Counts of term by category\n",
    "countvec = CountVectorizer(ngram_range=(1,3))\n",
    "def get_term_counts(x,category):\n",
    "    \n",
    "    # Split input text by unigram, bigram and trigram\n",
    "    # as the keywords may span up to 3 words\n",
    "    def get_ngram_text(x):\n",
    "        \n",
    "        try:\n",
    "            countvec.fit_transform(x)\n",
    "            text_list = countvec.get_feature_names()\n",
    "            return text_list\n",
    "\n",
    "        except ValueError:\n",
    "            return [' '] # to handle scenario where text input are all stop words only\n",
    "    \n",
    "    # check the existence of word by category\n",
    "    term_category = [t for t in get_ngram_text(x) if t in category]\n",
    "    \n",
    "    # return the number of occurence\n",
    "    return len(term_category)\n",
    "\n",
    "\n",
    "# Import external list, store as list\n",
    "term_absolute_list = get_term_list(\"term_list/compiled_absolute.txt\")\n",
    "term_allness_list = get_term_list(\"term_list/compiled_allness.txt\")\n",
    "term_badword_list = get_term_list(\"term_list/compiled_badword.txt\")\n",
    "term_negation_list = get_term_list(\"term_list/compiled_negation.txt\")\n",
    "term_diminisher_list = get_term_list(\"term_list/compiled_diminisher.txt\")\n",
    "term_intensifier_list = get_term_list(\"term_list/compiled_intensifier.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbd4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "def text_preprocessing_pipeline(df=bully_data,\n",
    "                                textual_statistics=False,\n",
    "                                remove_url=False,\n",
    "                                remove_email=False,\n",
    "                                remove_user_mention=False,\n",
    "                                remove_html=False,\n",
    "                                remove_space_single_char=False,\n",
    "                                normalize_elongated_char=False,\n",
    "                                normalize_emoji=False,\n",
    "                                normalize_emoticon=False,\n",
    "                                normalize_accented=False,\n",
    "                                lower_case=False,\n",
    "                                normalize_slang=False,\n",
    "                                normalize_badterm=False,\n",
    "                                spelling_check=False,\n",
    "                                normalize_contraction=False,\n",
    "                                term_list=False,\n",
    "                                remove_numeric=False,\n",
    "                                remove_stopword=False,\n",
    "                                keep_pronoun=False,\n",
    "                                remove_punctuation=False,\n",
    "                                pos=False,\n",
    "                                ner=False,\n",
    "                                lemmatise=False\n",
    "                               ):\n",
    "    '''\n",
    "    -------------\n",
    "     Description\n",
    "    -------------\n",
    "    Function that compile all preprocessing steps in one go\n",
    "    \n",
    "    -----------\n",
    "     Parameter\n",
    "    -----------\n",
    "    df: Data Frame\n",
    "    textual_statistics: Boolean\n",
    "    remove_url: Boolean\n",
    "    remove_email: Boolean\n",
    "    remove_user_mention: Boolean\n",
    "    remove_html: Boolean\n",
    "    remove_space_single_char: Boolean\n",
    "    normalize_elongated_char: Boolean\n",
    "    normalize_emoji: Boolean\n",
    "    normalize_emoticon: Boolean\n",
    "    normalize_accented: Boolean\n",
    "    lower_case: Boolean\n",
    "    normalize_slang: Boolean\n",
    "    normalize_badterm: Boolean\n",
    "    spelling_check: Boolean\n",
    "    normalize_contraction: Boolean\n",
    "    remove_numeric: Boolean\n",
    "    remove_stopword: Boolean\n",
    "    keep_pronoun: Boolean\n",
    "    remove_punctuation: Boolean\n",
    "    pos: Boolean\n",
    "    ner: Boolean\n",
    "    lemmatise: Boolean\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if textual_statistics:\n",
    "        print('Developing textual statistics from original text')\n",
    "        df['word_count'] = df['text'].progress_apply(lambda x: pt.get_wordcounts(x))\n",
    "        df['char_count'] = df['text'].progress_apply(lambda x: pt.get_char_counts(x))\n",
    "        df['avg_word_len'] = df['text'].progress_apply(lambda x: pt.get_avg_wordlength(x))\n",
    "        df['stopword_count'] = df['text'].progress_apply(lambda x: pt.get_stopwords_counts(x))\n",
    "        df['hashtag_count'] = df['text'].progress_apply(lambda x: pt.get_hashtag_counts(x))\n",
    "        df['mention_count'] = df['text'].progress_apply(lambda x: pt.get_mention_counts(x))\n",
    "        df['digit_counts'] = df['text'].progress_apply(lambda x: pt.get_digit_counts(x))\n",
    "        df['uppercase_count'] = df['text'].progress_apply(lambda x: pt.get_uppercase_counts(x))\n",
    "        df['emails_count'] = df['text'].progress_apply(lambda x: pt.get_emails(x))\n",
    "        df['urls_count'] = df['text'].progress_apply(lambda x: pt.get_urls(x))\n",
    "        df['punc_count'] = df['text'].progress_apply(lambda x: pt.get_punc_counts(x))\n",
    "        df[\"exclaimation_count\"] = df[\"text\"].progress_apply(lambda x: x.count(\"!\"))\n",
    "        df[\"questionmark_count\"] = df[\"text\"].progress_apply(lambda x: x.count(\"?\"))\n",
    "    \n",
    "    if pos:\n",
    "        print('Text Preprocessing: Developing POS tag count')\n",
    "        df[\"pos\"] = df[\"text\"].progress_apply(lambda x: pt.get_pos_tag(x))\n",
    "        df[\"pos_ADJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADJ\"))     #adjective\n",
    "        df[\"pos_ADP_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADP\"))     #adposition\n",
    "        df[\"pos_ADV_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"ADV\"))     #adverb\n",
    "        df[\"pos_AUX_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"AUX\"))     #auxiliary\n",
    "        df[\"pos_CCONJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"CCONJ\")) #coordinating conjunction\n",
    "        df[\"pos_DET_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"DET\"))     #determiner\n",
    "        df[\"pos_NOUN_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"NOUN\"))   #noun\n",
    "        df[\"pos_INTJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"INTJ\"))   #interjection\n",
    "        df[\"pos_NUM_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"NUM\"))     #numeral\n",
    "        df[\"pos_PART_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PART\"))   #particle\n",
    "        df[\"pos_PRON_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PRON\"))   #pronoun\n",
    "        df[\"pos_PROPN_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PROPN\")) #proper noun\n",
    "        df[\"pos_PUNCT_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"PUNCT\")) #punctuation\n",
    "        df[\"pos_SCONJ_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"SCONJ\")) #subordinating conjunction\n",
    "        df[\"pos_SYM_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"SYM\"))     #symbol\n",
    "        df[\"pos_VERB_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"VERB\"))   #verb\n",
    "        df[\"pos_other_counts\"] = df[\"pos\"].progress_apply(lambda x: pt.get_pos_tag_counts(x,pos_tag=\"X\"))     #other\n",
    "    \n",
    "    if ner:\n",
    "        print('Text Preprocessing: Developing NER tag count')\n",
    "        df[\"ner\"] = df[\"text\"].progress_apply(lambda x: pt.get_ner(x))\n",
    "        ner_lst = nlp.pipe_labels['ner']\n",
    "        for ner in ner_lst:\n",
    "             df[\"ner_\"+ ner +\"_counts\"] =  df[\"ner\"].apply(lambda x: pt.get_ner_counts(x,ner))\n",
    "                \n",
    "    if remove_url:\n",
    "        print('Text Preprocessing: Remove URL')\n",
    "        df['text_check'] = df['text'].progress_apply(lambda x: pt.remove_urls(x))\n",
    "        \n",
    "    if remove_email:\n",
    "        print('Text Preprocessing: Remove email')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_emails(x))\n",
    "        \n",
    "    if remove_user_mention:\n",
    "        print('Text Preprocessing: Remove user mention')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_mention(x))\n",
    "    \n",
    "    if remove_html:\n",
    "        print('Text Preprocessing: Remove html element')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_html_tags(x))\n",
    "        \n",
    "    if remove_space_single_char:\n",
    "        print('Text Preprocessing: Remove single spcae between single characters e.g F U C K')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_space_single_chars(x))\n",
    "        \n",
    "    if normalize_elongated_char:\n",
    "        print('Text Preprocessing: Reduction of elongated characters')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_elongated_chars(x))\n",
    "        \n",
    "    if normalize_emoji:\n",
    "        print('Text Preprocessing: Normalize and count emoji')\n",
    "        df['emoji_counts'] = df['text_check'].progress_apply(lambda x: pt.get_emoji_counts(x))\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.convert_emojis(x))\n",
    "        \n",
    "        \n",
    "    if normalize_emoticon:\n",
    "        print('Text Preprocessing: Normalize and count emoticon')\n",
    "        df['emoticon_counts'] = df['text_check'].progress_apply(lambda x: pt.get_emoticon_counts(x))\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.convert_emoticons(x))\n",
    "        \n",
    "        \n",
    "    if normalize_accented:\n",
    "        print('Text Preprocessing: Normalize accented character')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_accented_chars(x))\n",
    "        \n",
    "    if lower_case:\n",
    "        print('Text Preprocessing: Convert to lower case')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: str(x).lower())\n",
    "    \n",
    "    if normalize_slang:\n",
    "        print('Text Preprocessing: Normalize slang')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.slang_resolution(x))\n",
    "        \n",
    "    if normalize_badterm:\n",
    "        print('Text Preprocessing: Replace obfuscated bad term')\n",
    "        # unique words in vocab \n",
    "        unique_words = get_vocab(corpus= df['text_check'])\n",
    "        \n",
    "        # creating mapping dict \n",
    "        mapping_dict = create_profane_mapping(profane_words=term_badword_list,vocabulary=unique_words)\n",
    "        \n",
    "        df['text_check'] = replace_words(corpus=df['text_check'],\n",
    "                                                 mapping_dict=mapping_dict)\n",
    "        \n",
    "    if spelling_check:\n",
    "        print('Text Preprocessing: Spelling Check')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: tool.correct(x))\n",
    "        tool.close()\n",
    "        \n",
    "    if normalize_contraction:\n",
    "        print('Text Preprocessing: Contraction to Expansion')\n",
    "        \n",
    "        # Special handling to prevent code from taking forever to run\n",
    "        hardcode_clean_50702 = df['text_check'].iloc[50702].replace(\"'d\",\" would\").replace(\"wasn't\",\"was not\").replace(\"wouldn't\",\"would not\").replace(\"'s\",\" is\").replace(\"'m\",\" am\")\n",
    "        df['text_check'].iloc[50702] = hardcode_clean_50702\n",
    "\n",
    "        hardcode_clean_107720 = df['text_check'].iloc[107720].replace(\"'d\",\" would\").replace(\"wasn't\",\"was not\").replace(\"wouldn't\",\"would not\")\n",
    "        df['text_check'].iloc[107720] = hardcode_clean_107720\n",
    "\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: expand_contractions_with_embeddings(x, glove_model))\n",
    "\n",
    "    if remove_numeric: \n",
    "        print('Text Preprocessing: Remove numeric')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_numeric(x))\n",
    "        \n",
    "    if remove_punctuation:\n",
    "        print('Text Preprocessing: Remove punctuations')\n",
    "        df['text_check'] = df['text_check'].progress_apply(lambda x: pt.remove_special_chars(x))\n",
    "        \n",
    "    if remove_stopword:\n",
    "        print('Text Preprocessing: Remove stopword')\n",
    "        if keep_pronoun:\n",
    "            print('Text Preprocessing: and, keep Pronoun')\n",
    "        df[\"text_check\"] = df[\"text_check\"].progress_apply(lambda x: pt.remove_stopwords(x,keep_pronoun=keep_pronoun))\n",
    "        \n",
    "    # Remove multiple spaces\n",
    "    print('Text Preprocessing: Remove multiple spaces')\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n",
    "    \n",
    "    if lemmatise:\n",
    "        print('Text Preprocessing: Lemmatization')\n",
    "        df[\"text_check\"] = df[\"text_check\"].progress_apply(lambda x: pt.make_base(x))\n",
    "        \n",
    "    # Make sure remove multiple spaces\n",
    "    # df['text_check'] = df['text_check'].progress_apply(lambda x: ' '.join(x.split()))\n",
    "    \n",
    "    # Make sure lower case for all again\n",
    "    df['text_check'] = df['text_check'].progress_apply(lambda x: str(x).lower())\n",
    "    \n",
    "    # Remove empty text after cleaning\n",
    "    print('Last Step: Remove empty text after preprocessing. Done')\n",
    "    df = df[~df['text_check'].isna()]\n",
    "    df = df[df['text_check'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183c661",
   "metadata": {},
   "source": [
    "## Output the preprocessed and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82205cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing textual statistics from original text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e0052a07454f448792e66bda4c3036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cfbedb3cc34cd3b4809cdbf17092ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bdbbfe76e34a29bdc4385c05f8313a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200cf701a81949ae91e425cbff9fef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2094e95f7d6e48339a60465228c533bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0848e11b766b4c09985504a7d3236567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a81717b266c44049a8333af98762c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe9a9f251e49a3b429afe97ac9dd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43e877374ad4d28bf6fc03330ee29f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506139f3bc9e4d84ac894adad95d6dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba5310f6c094a7aa8a969fa546594cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb48f49aad4048e6a4516ca786475e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7935503970d45f9ade9d28c6d3b1c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Developing POS tag count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64424aad70bb4060968a15510474fd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a95613dfee49c99421d65c620ad29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9404866dacba4d4284f243c3f3ba0eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ea78d44bfc44f3a5d0c62d79d628b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254a601e2cb84822b29c4b902314a870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7736134fc144670aa616eee2666f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed1dee3663343499e1f62ca5c88fb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22da9d1b47c7466994b822fafd5ee581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3da38487de476fa5ce520d9231fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d9d884a30e4459a7359109a640af9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233de5d471d0415fb29ab333ff4cb283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d37ff413f04745a2a44e5f44fe2f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284093d87b374f8d804d5469e3eb6576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b998d37a2b1b47e9b711814e4b51beb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a051585f42496fb4f96d203dce38dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba39f9069a7b4c94881d45c5fb387edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368bfddd47284062a0e0b9aede871cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca6264012864fdfb25d064fa3501b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Developing NER tag count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd1d00a8ef04c79b6da8c2e419e78f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove URL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c005a05468334d3d8f422b8e01a95052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove email\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc51629b2d6442cba74ad3339568806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove user mention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f2a472a03848e9bb7c87d56beb1c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove html element\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b5dafd6fe84e5998c1be3028db7ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove single spcae between single characters e.g F U C K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7857e1627c4b62b66b50ce23a89957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Reduction of elongated characters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e8ed6d733c463a8f3b30f0086a8998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize and count emoji\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7dc0b49f044746ae47372b698df783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f156ad95f03486cac7abc601892a510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize and count emoticon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa749218bca4ee484dca3ba126fb8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdff37df77c4c64bddbe68fbca149a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize accented character\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe02149b0f1f4045b4f9f6b6ffc1f9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Convert to lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc11e90ffad4794b07eed9f2c0b6342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Normalize slang\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f595bca6b28c408b8f4b732e15bd2249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Replace obfuscated bad term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123548/123548 [00:00<00:00, 236306.14it/s]\n",
      "100%|██████████| 1921/1921 [1:02:39<00:00,  1.96s/it]\n",
      "100%|██████████| 123548/123548 [02:52<00:00, 718.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Spelling Check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12473d593d47ac8fbf18e02e7eba11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Contraction to Expansion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a054f37871e4d819aa904dc30ca8bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove numeric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c3597672704377aae011ca6febf116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove punctuations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c348fc43e8454b50b8f4c8b8be6acb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf7c4b3f58b47038097d97148e8d81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Lemmatization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c2c92f2a9f48f88f7f187ee1359c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2fff64f7bd4e6b833e905d49b7fa42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "bully_data_clean_with_stopword = text_preprocessing_pipeline(\n",
    "                                    df=bully_data,\n",
    "                                    textual_statistics=True,\n",
    "                                    remove_url=True,\n",
    "                                    remove_email=True,\n",
    "                                    remove_user_mention=True,\n",
    "                                    remove_html=True,\n",
    "                                    remove_space_single_char=True,\n",
    "                                    normalize_elongated_char=True,\n",
    "                                    normalize_emoji=True,\n",
    "                                    normalize_emoticon=True,\n",
    "                                    normalize_accented=True,\n",
    "                                    lower_case=True,\n",
    "                                    normalize_slang=True,\n",
    "                                    normalize_badterm=True,\n",
    "                                    spelling_check=True,\n",
    "                                    normalize_contraction=True,\n",
    "                                    remove_numeric=True,\n",
    "                                    remove_stopword=False, # Keep stopwords\n",
    "                                    keep_pronoun=False,  # Keep pronoun\n",
    "                                    remove_punctuation=True,\n",
    "                                    pos=True,\n",
    "                                    ner=True,\n",
    "                                    lemmatise=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19470ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bully_data_clean_with_stopword_base1 =  bully_data_clean_with_stopword.copy()\n",
    "bully_data_clean_with_stopword_base2 =  bully_data_clean_with_stopword.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be718eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove stopword\n",
      "Text Preprocessing: and, keep Pronoun\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9e9e72f7844185a7fe5a0909eabb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5128a7d3ab2149fbbeb198a21bbb4cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a451fac283a34b03b01c296c0ad374d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "bully_data_clean_no_stopword_pronoun = text_preprocessing_pipeline(\n",
    "                                            df=bully_data_clean_with_stopword_base1, \n",
    "                                            remove_stopword=True, # Remove stopwords\n",
    "                                            keep_pronoun=True) # But keep pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc007383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove stopword\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12a3a08ca6d45069f3c965578673944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing: Remove multiple spaces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df10db83ca40f3b3f77b8b933c9266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069933eb649e45aaaca0ea65a808786e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step: Remove empty text after preprocessing. Done\n"
     ]
    }
   ],
   "source": [
    "bully_data_clean_no_stopword_all = text_preprocessing_pipeline(\n",
    "                                        df=bully_data_clean_with_stopword_base2,\n",
    "                                        remove_stopword=True, # Remove all stopwords\n",
    "                                        keep_pronoun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab397fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bully_data_clean_with_stopword.to_csv('bully_data_clean_with_stopword.csv')\n",
    "bully_data_clean_no_stopword_pronoun.to_csv('bully_data_clean_no_stopword_pronoun.csv')\n",
    "bully_data_clean_no_stopword_all.to_csv('bully_data_clean_no_stopword_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
