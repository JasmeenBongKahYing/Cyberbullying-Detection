{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aff953",
   "metadata": {
    "id": "a0aff953"
   },
   "source": [
    "# Fine Tune Transformer from HuggingFace DistilElectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8dcc9",
   "metadata": {},
   "source": [
    "**Note: This notebook is run in Paperspace platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "OItEYDmL1hi2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:04.661264Z",
     "iopub.status.busy": "2023-08-13T07:38:04.660637Z",
     "iopub.status.idle": "2023-08-13T07:38:17.717754Z",
     "shell.execute_reply": "2023-08-13T07:38:17.716907Z",
     "shell.execute_reply.started": "2023-08-13T07:38:04.661240Z"
    },
    "id": "OItEYDmL1hi2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: imblearn in /usr/local/lib/python3.9/dist-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.9/dist-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.15.8)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install imblearn\n",
    "%pip install wandb --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02505ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:17.719967Z",
     "iopub.status.busy": "2023-08-13T07:38:17.719700Z",
     "iopub.status.idle": "2023-08-13T07:38:17.728424Z",
     "shell.execute_reply": "2023-08-13T07:38:17.727501Z",
     "shell.execute_reply.started": "2023-08-13T07:38:17.719941Z"
    },
    "id": "02505ed3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback, set_seed\n",
    "\n",
    "# Progress bar\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c1621",
   "metadata": {
    "id": "5a8c1621"
   },
   "source": [
    "## Import Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c58048aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:17.729410Z",
     "iopub.status.busy": "2023-08-13T07:38:17.729233Z",
     "iopub.status.idle": "2023-08-13T07:38:18.663950Z",
     "shell.execute_reply": "2023-08-13T07:38:18.663278Z",
     "shell.execute_reply.started": "2023-08-13T07:38:17.729393Z"
    },
    "id": "c58048aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29249e48f6949d2ac0ae2345f1223d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: Change the name of data set used for feature creation\n",
    "data_set='bully_data_clean_with_stopword'\n",
    "    \n",
    "    \n",
    "# Import Data Set #\n",
    "bully_data_cleaned = pd.read_csv('bully_data_clean_with_stopword.csv', encoding='utf8')                                            \n",
    "bully_data_cleaned = bully_data_cleaned[~bully_data_cleaned['text_check'].isna()]\n",
    "bully_data_cleaned = bully_data_cleaned[bully_data_cleaned['text_check'] != \"\"]\n",
    "#bully_data_cleaned = bully_data_cleaned[bully_data_cleaned['role']!='None']\n",
    "bully_data_cleaned = bully_data_cleaned[['label','text_check']]\n",
    "bully_data_cleaned['label'] = bully_data_cleaned['label'].progress_apply(lambda x: 1 if x ==\"Cyberbullying\" else 0)\n",
    "bully_data_cleaned.rename(columns = {'text_check':'text'}, inplace = True)\n",
    "bully_data_cleaned = bully_data_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1ad3546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:18.665918Z",
     "iopub.status.busy": "2023-08-13T07:38:18.665422Z",
     "iopub.status.idle": "2023-08-13T07:38:18.688745Z",
     "shell.execute_reply": "2023-08-13T07:38:18.687933Z",
     "shell.execute_reply.started": "2023-08-13T07:38:18.665891Z"
    },
    "id": "f1ad3546",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120932 entries, 0 to 120931\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   label   120932 non-null  int64 \n",
      " 1   text    120932 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "bully_data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "y2TxGLSm81wJ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:18.690447Z",
     "iopub.status.busy": "2023-08-13T07:38:18.689869Z",
     "iopub.status.idle": "2023-08-13T07:38:18.697330Z",
     "shell.execute_reply": "2023-08-13T07:38:18.696714Z",
     "shell.execute_reply.started": "2023-08-13T07:38:18.690389Z"
    },
    "id": "y2TxGLSm81wJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    115556\n",
       "1      5376\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bully_data_cleaned['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6da90",
   "metadata": {
    "id": "61d6da90"
   },
   "source": [
    "## Define pretrained tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b09bcba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:18.698443Z",
     "iopub.status.busy": "2023-08-13T07:38:18.698266Z",
     "iopub.status.idle": "2023-08-13T07:38:20.069220Z",
     "shell.execute_reply": "2023-08-13T07:38:20.068582Z",
     "shell.execute_reply.started": "2023-08-13T07:38:18.698427Z"
    },
    "id": "b09bcba2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8ad3f3e323c9bbccccb02a886541e18096e4439fe4bef351ad2a69b265245ebf.1b39ff9035b433c890d769efdd02fb72db0fcdc9429a75adbd43ca0f6edb2e5d\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"lsanochkin/distilelectra-base\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e147905218a02e7e6c15be0729fce12e50f1e4187ded38dd08eee0a0226ac9f7.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/37ad5df89bb584134b46596b5f4865371f123cd5318d788f37bf611c068018fc.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8ad3f3e323c9bbccccb02a886541e18096e4439fe4bef351ad2a69b265245ebf.1b39ff9035b433c890d769efdd02fb72db0fcdc9429a75adbd43ca0f6edb2e5d\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"lsanochkin/distilelectra-base\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8ad3f3e323c9bbccccb02a886541e18096e4439fe4bef351ad2a69b265245ebf.1b39ff9035b433c890d769efdd02fb72db0fcdc9429a75adbd43ca0f6edb2e5d\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"lsanochkin/distilelectra-base\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/lsanochkin/distilelectra-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51edf88376c75823badd8260a6b9c9a74b7aeceef4f5035c55bb324a39529d99.2135e6210ee7125ba7232471fd008dc2bcb3b6ee1b0e2bffdd6f468df1edecbc\n",
      "Some weights of the model checkpoint at lsanochkin/distilelectra-base were not used when initializing ElectraForSequenceClassification: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at lsanochkin/distilelectra-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"lsanochkin/distilelectra-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "set_seed(1127)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c063e",
   "metadata": {
    "id": "fe8c063e"
   },
   "source": [
    "## Preprocess data and Fine Tune Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5e716fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:20.070623Z",
     "iopub.status.busy": "2023-08-13T07:38:20.070302Z",
     "iopub.status.idle": "2023-08-13T07:38:48.524830Z",
     "shell.execute_reply": "2023-08-13T07:38:48.523957Z",
     "shell.execute_reply.started": "2023-08-13T07:38:20.070603Z"
    },
    "id": "b5e716fb"
   },
   "outputs": [],
   "source": [
    "# Data: Text Input and Label \n",
    "X = list(bully_data_cleaned[\"text\"])\n",
    "y = list(bully_data_cleaned[\"label\"])\n",
    "\n",
    "\n",
    "# Create torch dataset #\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Define Trainer parameters \n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall_cb = recall_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    precision_cb = precision_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    f1_cb = f1_score(y_true=labels, y_pred=pred, average='binary', pos_label=1)\n",
    "    \n",
    "    recall_ncb = recall_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    precision_ncb = precision_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    f1_ncb = f1_score(y_true=labels, y_pred=pred, average='binary', pos_label=0)\n",
    "    \n",
    "    recall_overall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision_overall = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1_overall = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "\n",
    "    return {\"accuracy\": accuracy, \n",
    "            \"precision_cb\": precision_cb, \"recall_cb\": recall_cb, \"f1_cb\": f1_cb,\n",
    "            \"precision_ncb\": precision_ncb, \"recall_ncb\": recall_ncb, \"f1_ncb\": f1_ncb,\n",
    "            \"precision_overall\": precision_overall, \"recall_overall\": recall_overall, \"f1_overall\": f1_overall}\n",
    "\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CONFUSION MATRIX IN PERCENTAGE\n",
    "    cf_pct = cf.astype('float')/cf.sum(axis=1)[:, np.newaxis]\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf_pct.flatten()]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf_pct,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "# Run cross-validation \n",
    "def run_cross_validation(model_name='DistilElectra',\n",
    "                         X=X,\n",
    "                         y=y, \n",
    "                         splits=5,\n",
    "                         epoch=4,\n",
    "                         checkpoint=False):\n",
    "    \n",
    "    kfold = StratifiedShuffleSplit(n_splits=splits, test_size=0.1, random_state=1127)\n",
    "    n_fold = 1\n",
    "\n",
    "    print(\"Developing Model with Cross validation for: \" + model_name)\n",
    "    for train, test in tqdm(kfold.split(X, y)):\n",
    "\n",
    "        print(\"Running for Fold: \",n_fold)\n",
    "        train_index = list(train)\n",
    "        test_index = list(test)\n",
    "\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        y_train = [y[i] for i in train_index]\n",
    "        X_val = [X[i] for i in test_index]\n",
    "        y_val = [y[i] for i in test_index]\n",
    "\n",
    "        # Tokenize\n",
    "        X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "        X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Create torch dataset\n",
    "        train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "        val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "        # Define Trainer\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"content/drive/MyDrive_binary/output_\" + model_name + \"/fold\"+str(n_fold),\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            num_train_epochs=epoch,\n",
    "            seed=1127,\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        )\n",
    "\n",
    "        if n_fold <= 4:\n",
    "            checkpoint_temp = True\n",
    "        else:\n",
    "            checkpoint_temp = False\n",
    "\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)  # Add gradient_clip_val\n",
    "        print(\"Complete for fold\", n_fold)\n",
    "        n_fold = n_fold + 1\n",
    "\n",
    "\n",
    "# Run Hold Out Test\n",
    "\n",
    "# Train and Test Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1127)\n",
    "\n",
    "# Train and Validate Set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=1127)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_test)\n",
    "\n",
    "def run_hold_out_split(model_name='DistilElectra',\n",
    "                       epoch=8,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=False):\n",
    "  \n",
    "    print(\"Developing Model with Hold Out Splits for: \" + model_name)\n",
    "    # Fine Tune Transformer\n",
    "    # Define Trainer\n",
    "    args = TrainingArguments(\n",
    "      output_dir=\"content/drive/MyDrive_binary/output_\" + model_name + \"/holdout\",\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\",\n",
    "      #eval_steps=500,\n",
    "      #per_device_train_batch_size=1,\n",
    "      #per_device_eval_batch_size=1,\n",
    "      num_train_epochs=epoch, #1 was okay\n",
    "      seed=1127,\n",
    "      load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    # model_init=model_init,\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"Complete for hold-out validate set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "EKrMcyWmEBlO",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:48.526846Z",
     "iopub.status.busy": "2023-08-13T07:38:48.526207Z",
     "iopub.status.idle": "2023-08-13T07:38:48.537263Z",
     "shell.execute_reply": "2023-08-13T07:38:48.536618Z",
     "shell.execute_reply.started": "2023-08-13T07:38:48.526817Z"
    },
    "id": "EKrMcyWmEBlO"
   },
   "outputs": [],
   "source": [
    "# Predict (Hold Out Test) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_metrics_holdout(model_name='DistilElectra',\n",
    "                            model_path='content/drive/MyDrive_binary/output_DistilElectra/holdout/checkpoint-3820', \n",
    "                            average_method='binary',\n",
    "                            X_test=X_test):\n",
    "  \n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # Load trained model\n",
    "    model_pred = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "    # Define test trainer\n",
    "    test_trainer = Trainer(model_pred)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "    # Preprocess raw predictions\n",
    "    y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision_cb = precision_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "    recall_cb = recall_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "    f1_cb = f1_score(y_test, y_pred, average=average_method, pos_label=1)\n",
    "\n",
    "    precision_ncb = precision_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "    recall_ncb = recall_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "    f1_ncb = f1_score(y_test, y_pred, average=average_method, pos_label=0)\n",
    "\n",
    "    precision_overall = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_overall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_overall = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Print Results\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print()\n",
    "    print(\"Label 1: Cyberbullying\")\n",
    "    print(\"Precision: \", precision_cb)\n",
    "    print(\"Recall: \", recall_cb)\n",
    "    print(\"F-measure: \", f1_cb)\n",
    "    print()\n",
    "    print(\"Label 0: Non-Cyberbullying\")\n",
    "    print(\"Precision: \", precision_ncb)\n",
    "    print(\"Recall: \", recall_ncb)\n",
    "    print(\"F-measure: \", f1_ncb)\n",
    "    print()\n",
    "    print(\"Macro Metrics\")\n",
    "    print(\"Precision: \", precision_overall)\n",
    "    print(\"Recall: \", recall_overall)\n",
    "    print(\"F-measure: \", f1_overall)\n",
    "    print()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_mat = confusion_matrix(y_test,y_pred)\n",
    "    categories = ['Non-Cyberbullying', 'Cyberbullying']\n",
    "    labels = ['True Negative','',\n",
    "            '','True Positive']\n",
    "\n",
    "    make_confusion_matrix(conf_mat, \n",
    "                        group_names=labels,\n",
    "                        categories=categories, \n",
    "                        figsize=(8,5), \n",
    "                        cbar=True, \n",
    "                        title='Fine Tuned ' + model_name + ' for Cyberbullying Detection', \n",
    "                        cmap='YlGnBu', \n",
    "                        sum_stats=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5A2sV2dKlPb",
   "metadata": {
    "id": "q5A2sV2dKlPb",
    "tags": []
   },
   "source": [
    "## Cross Validation (K = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b495d5-270f-4f8c-a479-90a75f0e459e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-13T07:38:48.539204Z",
     "iopub.status.busy": "2023-08-13T07:38:48.538479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Model with Cross validation for: DistilElectra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Fold:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 108838\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27212\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27212' max='27212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27212/27212 1:54:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Cb</th>\n",
       "      <th>Recall Cb</th>\n",
       "      <th>F1 Cb</th>\n",
       "      <th>Precision Ncb</th>\n",
       "      <th>Recall Ncb</th>\n",
       "      <th>F1 Ncb</th>\n",
       "      <th>Precision Overall</th>\n",
       "      <th>Recall Overall</th>\n",
       "      <th>F1 Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.109363</td>\n",
       "      <td>0.964941</td>\n",
       "      <td>0.623913</td>\n",
       "      <td>0.533457</td>\n",
       "      <td>0.575150</td>\n",
       "      <td>0.978425</td>\n",
       "      <td>0.985029</td>\n",
       "      <td>0.981716</td>\n",
       "      <td>0.801169</td>\n",
       "      <td>0.759243</td>\n",
       "      <td>0.778433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.966926</td>\n",
       "      <td>0.643154</td>\n",
       "      <td>0.576208</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.980365</td>\n",
       "      <td>0.985116</td>\n",
       "      <td>0.982735</td>\n",
       "      <td>0.811759</td>\n",
       "      <td>0.780662</td>\n",
       "      <td>0.795289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.111184</td>\n",
       "      <td>0.968331</td>\n",
       "      <td>0.684086</td>\n",
       "      <td>0.535316</td>\n",
       "      <td>0.600626</td>\n",
       "      <td>0.978583</td>\n",
       "      <td>0.988491</td>\n",
       "      <td>0.983512</td>\n",
       "      <td>0.831334</td>\n",
       "      <td>0.761903</td>\n",
       "      <td>0.792069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.116585</td>\n",
       "      <td>0.968745</td>\n",
       "      <td>0.680180</td>\n",
       "      <td>0.561338</td>\n",
       "      <td>0.615071</td>\n",
       "      <td>0.979742</td>\n",
       "      <td>0.987712</td>\n",
       "      <td>0.983711</td>\n",
       "      <td>0.829961</td>\n",
       "      <td>0.774525</td>\n",
       "      <td>0.799391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-6803\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-6803/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-6803/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-13606\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-13606/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-13606/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-20409\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-20409/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-20409/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-27212\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-27212/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-27212/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from content/drive/MyDrive_binary/output_DistilElectra/fold1/checkpoint-6803 (score: 0.10936307907104492).\n",
      "1it [1:54:16, 6856.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete for fold 1\n",
      "Running for Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 108838\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27212\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27212' max='27212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27212/27212 1:52:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Cb</th>\n",
       "      <th>Recall Cb</th>\n",
       "      <th>F1 Cb</th>\n",
       "      <th>Precision Ncb</th>\n",
       "      <th>Recall Ncb</th>\n",
       "      <th>F1 Ncb</th>\n",
       "      <th>Precision Overall</th>\n",
       "      <th>Recall Overall</th>\n",
       "      <th>F1 Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.097090</td>\n",
       "      <td>0.971639</td>\n",
       "      <td>0.732697</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.641588</td>\n",
       "      <td>0.980214</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>0.985235</td>\n",
       "      <td>0.856456</td>\n",
       "      <td>0.780470</td>\n",
       "      <td>0.813412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.099585</td>\n",
       "      <td>0.972135</td>\n",
       "      <td>0.746929</td>\n",
       "      <td>0.565056</td>\n",
       "      <td>0.643386</td>\n",
       "      <td>0.979978</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>0.985501</td>\n",
       "      <td>0.863453</td>\n",
       "      <td>0.778071</td>\n",
       "      <td>0.814444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.109311</td>\n",
       "      <td>0.972052</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.576208</td>\n",
       "      <td>0.647182</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.990481</td>\n",
       "      <td>0.985450</td>\n",
       "      <td>0.859282</td>\n",
       "      <td>0.783345</td>\n",
       "      <td>0.816316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.114097</td>\n",
       "      <td>0.970977</td>\n",
       "      <td>0.721040</td>\n",
       "      <td>0.566914</td>\n",
       "      <td>0.634755</td>\n",
       "      <td>0.980036</td>\n",
       "      <td>0.989789</td>\n",
       "      <td>0.984888</td>\n",
       "      <td>0.850538</td>\n",
       "      <td>0.778352</td>\n",
       "      <td>0.809822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-6803\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-6803/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-6803/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-13606\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-13606/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-13606/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-20409\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-20409/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-20409/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-27212\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-27212/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-27212/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from content/drive/MyDrive_binary/output_DistilElectra/fold2/checkpoint-6803 (score: 0.09708995372056961).\n",
      "2it [3:47:02, 6803.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete for fold 2\n",
      "Running for Fold:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 108838\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27212\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13110' max='27212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13110/27212 36:08 < 38:53, 6.04 it/s, Epoch 1.93/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Cb</th>\n",
       "      <th>Recall Cb</th>\n",
       "      <th>F1 Cb</th>\n",
       "      <th>Precision Ncb</th>\n",
       "      <th>Recall Ncb</th>\n",
       "      <th>F1 Ncb</th>\n",
       "      <th>Precision Overall</th>\n",
       "      <th>Recall Overall</th>\n",
       "      <th>F1 Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.124190</td>\n",
       "      <td>0.972548</td>\n",
       "      <td>0.828025</td>\n",
       "      <td>0.483271</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.976401</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.985773</td>\n",
       "      <td>0.902213</td>\n",
       "      <td>0.739299</td>\n",
       "      <td>0.798051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12094\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to content/drive/MyDrive_binary/output_DistilElectra/fold3/checkpoint-6803\n",
      "Configuration saved in content/drive/MyDrive_binary/output_DistilElectra/fold3/checkpoint-6803/config.json\n",
      "Model weights saved in content/drive/MyDrive_binary/output_DistilElectra/fold3/checkpoint-6803/pytorch_model.bin\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation(model_name='DistilElectra',\n",
    "                         X=X,\n",
    "                         y=y, \n",
    "                         splits=5,\n",
    "                         epoch=4,\n",
    "                         checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cabdda-c5a2-44ef-b8ef-61de87bc9003",
   "metadata": {
    "id": "q5A2sV2dKlPb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [Continue] Cross Validation (K = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a158e0-251b-47f3-80bb-f5a47f2496a0",
   "metadata": {
    "id": "p5QGotWv8_-V"
   },
   "outputs": [],
   "source": [
    "run_cross_validation(model_name='DistilElectra',\n",
    "                         X=X,\n",
    "                         y=y, \n",
    "                         splits=5,\n",
    "                         epoch=4,\n",
    "                         checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W4F4117sKwZQ",
   "metadata": {
    "id": "W4F4117sKwZQ"
   },
   "source": [
    "## Hold Out Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37e94d-0b71-406e-b0d3-e2e6c6ead9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "run_hold_out_split(model_name='DistilElectra',\n",
    "                       epoch=4,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb07800-22ad-4740-8778-ff3922c37e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "run_hold_out_split(model_name='DistilElectra',\n",
    "                       epoch=4,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=val_dataset,\n",
    "                       checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w5vyE3EkK2JQ",
   "metadata": {
    "id": "w5vyE3EkK2JQ",
    "tags": []
   },
   "source": [
    "### Test Split Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fG7eAEaFtIYk",
   "metadata": {
    "id": "fG7eAEaFtIYk"
   },
   "source": [
    "#### Epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06398b",
   "metadata": {
    "id": "ab06398b"
   },
   "outputs": [],
   "source": [
    "compute_metrics_holdout(model_name='DistilElectra',\n",
    "                        model_path='content/drive/MyDrive_binary/output_DistilElectra/holdout/checkpoint-6803', \n",
    "                        average_method='binary',\n",
    "                        X_test=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ir4eL3T8gtLf",
   "metadata": {
    "id": "Ir4eL3T8gtLf"
   },
   "source": [
    "#### Epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u34gKYLGgtLr",
   "metadata": {
    "id": "u34gKYLGgtLr"
   },
   "outputs": [],
   "source": [
    "compute_metrics_holdout(model_name='DistilElectra',\n",
    "                        model_path='content/drive/MyDrive_binary/output_DistilElectra/holdout/checkpoint-13606', \n",
    "                        average_method='binary',\n",
    "                        X_test=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KdECewOiL4tf",
   "metadata": {
    "id": "KdECewOiL4tf"
   },
   "source": [
    "#### Epoch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rj2PqaNBL4tg",
   "metadata": {
    "id": "Rj2PqaNBL4tg"
   },
   "outputs": [],
   "source": [
    "compute_metrics_holdout(model_name='DistilElectra',\n",
    "                        model_path='content/drive/MyDrive_binary/output_DistilElectra/holdout/checkpoint-20409', \n",
    "                        average_method='binary',\n",
    "                        X_test=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uFLTBUzxL4MC",
   "metadata": {
    "id": "uFLTBUzxL4MC"
   },
   "source": [
    "#### Epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MrkHnZgHL4MC",
   "metadata": {
    "id": "MrkHnZgHL4MC"
   },
   "outputs": [],
   "source": [
    "compute_metrics_holdout(model_name='DistilElectra',\n",
    "                        model_path='content/drive/MyDrive_binary/output_DistilElectra/holdout/checkpoint-27212', \n",
    "                        average_method='binary',\n",
    "                        X_test=X_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "t6kQkR3fK9T0",
    "fG7eAEaFtIYk",
    "Ir4eL3T8gtLf",
    "KdECewOiL4tf",
    "uFLTBUzxL4MC",
    "eFNCqLf6L3pO",
    "OYWrSlPXL3Ac",
    "jhRILtDlL2XU",
    "OC_9eI5WMF8p",
    "5CH0cmxrMFSV",
    "xgLbAOZ7MEdd",
    "wTPokVq2pcUu",
    "5vlEdGyfpcUv",
    "3SHI4jYYpcUv",
    "jptg0BcZpcUv",
    "_AlZhKF5pcUv",
    "6MAeDeTWpcUv",
    "yqeXDiEipcUw",
    "ArQmCEzspcUw",
    "oDn0CPRXpcUw",
    "mYNtqY5cpcUw"
   ],
   "name": "Fine Tune Transformers from HuggingFace RoBerta (Sample).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
